{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzSPdsFJxF1Vrk5GhWzTMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Building-Generative-AI-Powered-Applications-with-Python/blob/main/Give_Meaningful_Names_to_Your_Photos_with_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction ###\n"
      ],
      "metadata": {
        "id": "PplQ3qob2zBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images, rich with untapped information, often come under the radar of search engines and data systems. Transforming this visual data into machine-readable language is no easy task, but it's where image captioning AI is useful. Here's how image captioning AI can make a difference:\n",
        "\n",
        "* Improves accessibility: Helps visually impaired individuals understand visual content.\n",
        "* Enhances SEO: Assists search engines in identifying the content of images.\n",
        "* Facilitates content discovery: Enables efficient analysis and categorization of large image databases.\n",
        "* Supports social media and advertising: Automates engaging description generation for visual content.\n",
        "* Boosts security: Provides real-time descriptions of activities in video footage.\n",
        "* Aids in education and research: Assists in understanding and interpreting visual materials.\n",
        "* Offers multilingual support: Generates image captions in various languages for international audiences.\n",
        "* Enables data organization: Helps manage and categorize large sets of visual data.\n",
        "* Saves time: Automated captioning is more efficient than manual efforts.\n",
        "* Increases user engagement: Detailed captions can make visual content more engaging and informative."
      ],
      "metadata": {
        "id": "R9gl-5pr23tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning objectives ###"
      ],
      "metadata": {
        "id": "wGikhB3W3CHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this project, you will be able to:\n",
        "\n",
        "* Implement an image captioning tool using the BLIP model from Hugging Face's Transformers\n",
        "\n",
        "* Use Gradio to provide a user-friendly interface for your image captioning application\n",
        "\n",
        "* Adapt the tool for real-world business scenarios, demonstrating its practical applications"
      ],
      "metadata": {
        "id": "gUmVcu_Z3XYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRZoiDXeiPy_"
      },
      "outputs": [],
      "source": [
        "!pip install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv my_env # create a virtual environment my_env"
      ],
      "metadata": {
        "id": "u3J2m4FJihZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source my_env/bin/activate # activate my_env"
      ],
      "metadata": {
        "id": "J_RXb10RiuvJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "hKVE-dRk4Yxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required libraries in my_env\n",
        "!pip install langchain gradio==5.23.2 transformers==4.41.0 bs4==0.0.2 requests==2.32.3 torch==2.6.0"
      ],
      "metadata": {
        "id": "fWqwbXeSizR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "# Load the pretrained processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMVz_viM1FRx",
        "outputId": "eabb366e-dfa7-4097-b198-23e9568c18a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your image, DONT FORGET TO WRITE YOUR IMAGE NAME\n",
        "img_path = \"lion.jpg\"\n",
        "# convert it into an RGB format\n",
        "image = Image.open(img_path).convert('RGB')"
      ],
      "metadata": {
        "id": "l8-MOEMJ1lQv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a caption for the image\n",
        "inputs = processor(image, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)"
      ],
      "metadata": {
        "id": "PnGw_qu71zuT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated tokens to text\n",
        "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "# Print the caption\n",
        "print(caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFJFKcZ_2YKg",
        "outputId": "a4b61cba-75e5-4b2f-b27e-bc75595a4d00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a lion laying in the grass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image captioning app with Gradio ###"
      ],
      "metadata": {
        "id": "T31xA1Eq3hHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "tPtE-f6w3l4E",
        "outputId": "81888058-22e9-45e5-e35e-f5e41368a7cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1186ca01df9fee75ab.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1186ca01df9fee75ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement image captioning app with Gradio ###"
      ],
      "metadata": {
        "id": "7Bij5c8t6Ssz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "# Load the pretrained processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "# URL of the page to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/IBM\"\n",
        "# Download the page\n",
        "response = requests.get(url)\n",
        "# Parse the page with BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Find all img elements\n",
        "img_elements = soup.find_all('img')\n",
        "# Open a file to write the captions\n",
        "with open(\"captions.txt\", \"w\") as caption_file:\n",
        "    # Iterate over each img element\n",
        "    for img_element in img_elements:\n",
        "        img_url = img_element.get('src')\n",
        "        # Skip if the image is an SVG or too small (likely an icon)\n",
        "        if 'svg' in img_url or '1x1' in img_url:\n",
        "            continue\n",
        "        # Correct the URL if it's malformed\n",
        "        if img_url.startswith('//'):\n",
        "            img_url = 'https:' + img_url\n",
        "        elif not img_url.startswith('http://') and not img_url.startswith('https://'):\n",
        "            continue  # Skip URLs that don't start with http:// or https://\n",
        "        try:\n",
        "            # Download the image\n",
        "            response = requests.get(img_url)\n",
        "            # Convert the image data to a PIL Image\n",
        "            raw_image = Image.open(BytesIO(response.content))\n",
        "            if raw_image.size[0] * raw_image.size[1] < 400:  # Skip very small images\n",
        "                continue\n",
        "            raw_image = raw_image.convert('RGB')\n",
        "            # Process the image\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "            # Generate a caption for the image\n",
        "            out = model.generate(**inputs, max_new_tokens=50)\n",
        "            # Decode the generated tokens to text\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "            # Write the caption to the file, prepended by the image URL\n",
        "            caption_file.write(f\"{img_url}: {caption}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_url}: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "w-GzSY1v6Utn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image captioning for local files (Run locally if using Blip2) ###"
      ],
      "metadata": {
        "id": "7RqBrAtu8SVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration #Blip2 models\n",
        "# Load the pretrained processor and model\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "# Specify the directory where your images are\n",
        "image_dir = \"/path/to/your/images\"\n",
        "image_exts = [\"jpg\", \"jpeg\", \"png\"]  # specify the image file extensions to search for\n",
        "# Open a file to write the captions\n",
        "with open(\"captions.txt\", \"w\") as caption_file:\n",
        "    # Iterate over each image file in the directory\n",
        "    for image_ext in image_exts:\n",
        "        for img_path in glob.glob(os.path.join(image_dir, f\"*.{image_ext}\")):\n",
        "            # Load your image\n",
        "            raw_image = Image.open(img_path).convert('RGB')\n",
        "            # You do not need a question for image captioning\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "            # Generate a caption for the image\n",
        "            out = model.generate(**inputs, max_new_tokens=50)\n",
        "            # Decode the generated tokens to text\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "            # Write the caption to the file, prepended by the image file name\n",
        "            caption_file.write(f\"{os.path.basename(img_path)}: {caption}\\n\")"
      ],
      "metadata": {
        "id": "kauLXHxq8Rbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}