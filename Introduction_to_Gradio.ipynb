{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWmI27Ry30/PIqm7qkgQTv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Building-Generative-AI-Powered-Applications-with-Python/blob/main/Introduction_to_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction ###\n",
        "Gradio is the way to demonstrate your machine learning model with a user-friendly web interface so that everyone can use it anywhere. Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience is needed."
      ],
      "metadata": {
        "id": "xVXAN6O2AutE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Use Gradio? ###\n",
        "Gradio is useful for several reasons:\n",
        "\n",
        "Ease of use: Gradio enables the creation of interfaces for models with just a few lines of code.\n",
        "Flexibility: Gradio supports various inputs and outputs, such as text, images, files, and more.\n",
        "Sharing and collaboration: Interfaces can be shared with others through unique URLs, facilitating easy collaboration and feedback collection."
      ],
      "metadata": {
        "id": "fCuYYj6iAzY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started with Gradio ###\n",
        "To begin using Gradio, you first need to install the library. You can install Gradio using pip:"
      ],
      "metadata": {
        "id": "brRAPjNiA3qE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5bZr4X1AOcP"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Your First Gradio Interface ###\n",
        "You can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:"
      ],
      "metadata": {
        "id": "OFdfQg93A_E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8MKUIoXqA3Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def greet(name, intensity):\n",
        "  return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "demo = gr.Interface(\n",
        "  fn=greet,\n",
        "  inputs=[\"text\", \"slider\"],\n",
        "  outputs=[\"text\"],\n",
        ")\n",
        "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "H1NQEbBNBGC1",
        "outputId": "0d0a4792-9105-4c19-e806-acb7f65f13fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4f2657b4553a36516.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4f2657b4553a36516.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right."
      ],
      "metadata": {
        "id": "E6oIAgSUBp-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Interface class ###\n",
        "Note that to make your first demo, you created an instance of the gr.Interface class. The Interface class is designed to create demos for machine learning models that accept one or more inputs and return one or more outputs.\n",
        "\n",
        "The Interface class has three core arguments:\n",
        "\n",
        "* fn: The function to wrap a user interface (UI) around\n",
        "* inputs: The Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n",
        "* outputs: The Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n",
        "\n",
        "The fn argument is flexible — you can pass any Python function you want to wrap with a UI. In the example above, you saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n",
        "\n",
        "The input and output arguments take one or more Gradio components. As we'll see, Gradio includes more than 30 built-in components (such as the gr.Textbox(), gr.Image(), and gr.HTML() components) that are designed for machine learning applications.\n",
        "\n",
        "If your function accepts more than one argument, as is the case above, pass a list of input components to inputs, with each input component corresponding to one of the function's arguments in order. The same applies if your function returns more than one value: simply pass a list of components to outputs. This flexibility makes the Interface class a very powerful way to create demos."
      ],
      "metadata": {
        "id": "mZvkFqXyBxIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a simple interface for an image captioning mode ###\n",
        "\n",
        "Let's create a simple interface for an image captioning model. The BLIP (Bootstrapped Language Image Pretraining) model can generate captions for images. Here's how you can create a Gradio interface for the BLIP model."
      ],
      "metadata": {
        "id": "cTdHX5rACG9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgYernfhCWcO",
        "outputId": "90d15bd3-0c88-4afa-c428-d286553046a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "def generate_caption(image):\n",
        "    # Now directly using the PIL Image object\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "def caption_image(image):\n",
        "    \"\"\"\n",
        "    Takes a PIL Image input and returns a caption.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        caption = generate_caption(image)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "iface = gr.Interface(\n",
        "    fn=caption_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Image Captioning with BLIP\",\n",
        "    description=\"Upload an image to generate a caption.\"\n",
        ")\n",
        "iface.launch(server_name=\"127.0.0.1\", server_port= 7862)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "_-xcQLbXCdwP",
        "outputId": "0cea4694-2e53-4413-9669-ee96aace64e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://63723c31873804e41d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://63723c31873804e41d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use the BlipProcessor and BlipForConditionalGeneration from the transformers q library to set up an image captioning model. This example demonstrates creating a web interface using Gradio, where the input parameter specifies an image and the output is the generated text caption. The title and description parameters enhance the interface by providing context and instructions for users."
      ],
      "metadata": {
        "id": "TEGrCcB7DPRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification in PyTorch ###\n",
        "Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n",
        "\n",
        "Such models are perfect to use with Gradio's image input component. In this tutorial, we will build a web demo to classify images using Gradio. We can build the whole web application in Python."
      ],
      "metadata": {
        "id": "psmp6I3yDZJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting up the image classification model ###\n",
        "First, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from PyTorch Hub. You can use a different pretrained model or train your own."
      ],
      "metadata": {
        "id": "3xcU27MTDfWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5hcsb_oDlNH",
        "outputId": "606837a3-3182-4aeb-ee11-ddbcabc6a557"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.6.0\" to /root/.cache/torch/hub/v0.6.0.zip\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 238MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Defining a predict function ###\n",
        "Next, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this text file.\n",
        "\n",
        "In the case of our pretrained model, it will look like this:"
      ],
      "metadata": {
        "id": "omTlhAJuDsPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")\n",
        "def predict(inp):\n",
        " inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        " with torch.no_grad():\n",
        "  prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
        "  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
        " return confidences"
      ],
      "metadata": {
        "id": "jUfziQNYDxHr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break this down.\n",
        "\n",
        "* The function takes one parameter:\n",
        "```python\n",
        "#inp: the input image as a PIL image\n",
        "```\n",
        "\n",
        "* The function converts the input image into a PIL Image and subsequently into a PyTorch tensor.\n",
        "* After processing the tensor through the model, it returns the predictions in the form of a dictionary named confidences.\n",
        "* The dictionary's keys are the class labels, and its values are the corresponding confidence probabilities.\n",
        "* In this section, we define a predict function that processes an input image to return prediction probabilities.\n",
        "* The function first converts the image into a PyTorch tensor and then forwards it through the pretrained model.\n",
        "* We use the softmax function in the final step to calculate the probabilities of each class. The softmax function is crucial because it converts the raw output logits from the model, which can be any real number, into probabilities that sum up to 1. This makes it easier to interpret the model's outputs as confidence levels for each class."
      ],
      "metadata": {
        "id": "KfcUKzykD35M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Creating a Gradio interface ###\n",
        "Now that we have our predictive function set up, we can create a Gradio Interface around it.\n",
        "\n",
        "In this case, the input component is a drag-and-drop image component. To create this input, we use\n",
        "\n",
        "```python\n",
        "Image(type=“pil”)\n",
        "```\n",
        "\n",
        "which creates the component and handles the preprocessing to convert that to a PIL image.\n",
        "\n",
        "The output component will be a Label, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as\n",
        "\n",
        "```python\n",
        "Label(num_top_classes=3)\n",
        "```\n",
        "\n",
        "Finally, we'll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:"
      ],
      "metadata": {
        "id": "H4JpJEUNEpX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "gr.Interface(fn=predict,\n",
        "       inputs=gr.Image(type=\"pil\"),\n",
        "       outputs=gr.Label(num_top_classes=3),\n",
        "       examples=[\"/content/lion.jpg\", \"/content/cheetah.jpg\"]).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "cT8_5v3wFDQE",
        "outputId": "9f149e56-4c1c-47b5-ca8d-bed084cbd337"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://391bca2cb1277b2f86.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://391bca2cb1277b2f86.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example paths provided, /content/lion.jpg and /content/cheetah.jpg, are placeholders.\n",
        "\n",
        "You should replace these with the actual paths to images on your system or server where you have saved the images you want to use for testing.\n",
        "\n",
        "This ensures that when you or others are using the Gradio interface, the examples are correctly loaded and can be used to demonstrate the functionality of your image classifier."
      ],
      "metadata": {
        "id": "PVOQ7t7uFX00"
      }
    }
  ]
}