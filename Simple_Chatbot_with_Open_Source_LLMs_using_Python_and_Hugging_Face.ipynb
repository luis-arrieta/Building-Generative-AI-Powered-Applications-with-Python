{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4vfL+IQbg4geUvVWDspfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Building-Generative-AI-Powered-Applications-with-Python/blob/main/Simple_Chatbot_with_Open_Source_LLMs_using_Python_and_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Simple Chatbot with Open Source LLMs using Python and Hugging Face ##\n",
        "\n",
        "### Learning outcomes: ###\n",
        "\n",
        "At the end of this lab, you will be able to:\n",
        "\n",
        "* Describe the main components of a chatbot\n",
        "* Explain what an LLM is\n",
        "* Select an LLM for your application\n",
        "* Describe how a transformer essentially works\n",
        "* Feed input into a transformer (tokenization)\n",
        "* Program your own simple chatbot in Python"
      ],
      "metadata": {
        "id": "TGJrqTk-AzII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Installing requirements ###\n",
        "\n",
        "Follow these steps to create a Python virtual environment and install the necessary libraries.\n",
        "\n",
        "Set up your virtual environment:"
      ],
      "metadata": {
        "id": "pQ1ILNUHBHpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIoss6TaAtAp"
      },
      "outputs": [],
      "source": [
        "!pip3 install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv my_env # create a virtual environment my_env"
      ],
      "metadata": {
        "id": "ZsCnTWPSBTwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source my_env/bin/activate # activate my_env"
      ],
      "metadata": {
        "id": "JMv_dYV7BVg-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import our required tools from the transformers library ###\n",
        "\n",
        "For this example, you will be using AutoTokenizer and AutoModelForSeq2SeqLM from the transformers library."
      ],
      "metadata": {
        "id": "DYPUV9bEBnsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "c2VMu-MGBsnU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Choosing a model ###\n",
        "\n",
        "Choosing the right model for your purposes is an important part of building chatbots! You can read on the different types of models available on the Hugging Face website: https://huggingface.co/models.\n",
        "\n",
        "LLMs differ from each other in how they are trained. Let's look at some examples to see how different models fit better in various contexts.\n",
        "\n",
        "* Text generation: If you need a general-purpose text generation model, consider using the GPT-2 or GPT-3 models. They are known for their impressive language generation capabilities.\n",
        "Example: You want to build a chatbot that generates creative and coherent responses to user input.\n",
        "\n",
        "* Sentiment analysis: For sentiment analysis tasks, models like BERT or RoBERTa are popular choices. They are trained to understand the sentiment and emotional tone of text.\n",
        "Example: You want to analyze customer feedback and determine whether it is positive or negative.\n",
        "\n",
        "* Named entity recognition: LLMs such as BERT, GPT-2, or RoBERTa can be used for Named Entity Recognition (NER) tasks. They perform well in understanding and extracting entities like person names, locations, organizations, etc.\n",
        "Example: You want to build a system that extracts names of people and places from a given text.\n",
        "\n",
        "* Question answering: Models like BERT, GPT-2, or XLNet can be effective for question-answering tasks. They can comprehend questions and provide accurate answers based on the given context.\n",
        "Example: You want to build a chatbot that can answer factual questions from a given set of documents.\n",
        "\n",
        "* Language translation: For language translation tasks, you can consider models like MarianMT or T5. They are designed specifically for translating text between different languages.\n",
        "Example: You want to build a language translation tool that translates English text to French.\n",
        "\n",
        "However, these examples are very limited and the fit of an LLM may depend on many factors such as data availability, performance requirements, resource constraints, and domain-specific considerations. It's important to explore different LLMs thoroughly and experiment with them to find the best match for your specific application.\n",
        "\n",
        "Other important purposes that should be taken into consideration when choosing an LLM include (but are not limited to):\n",
        "\n",
        "* Licensing: Ensure you are allowed to use your chosen model the way you intend\n",
        "\n",
        "* Model size: Larger models may be more accurate, but might also come at the cost of greater resource requirements\n",
        "\n",
        "* Training data: Ensure that the model's training data aligns with the domain or context you intend to use the LLM for\n",
        "\n",
        "* Performance and accuracy: Consider factors like accuracy, runtime, or any other metrics that are important for your specific use case\n",
        "\n",
        "To explore all the different options, check out the available models on the Hugging Face website.\n",
        "\n",
        "For this example, you'll be using facebook/blenderbot-400M-distill because it has an open-source license and runs relatively fast."
      ],
      "metadata": {
        "id": "41ZnvT2FB1vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/blenderbot-400M-distill\""
      ],
      "metadata": {
        "id": "MpNRMxyyB1Iw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Fetch the model and initialize a tokenizer ###\n",
        "When running this code for the first time, the host machine will download the model from Hugging Face API.\n",
        "However, after running the code once, the script will not re-download the model and will instead reference the local installation.\n",
        "\n",
        "You'll be looking at two terms here: model and tokenizer.\n",
        "\n",
        "In this script, you initiate variables using two handy classes from the transformers library:\n",
        "\n",
        "* model is an instance of the class AutoModelForSeq2SeqLM, which allows you to interact with your chosen language model.\n",
        "\n",
        "* tokenizer is an instance of the class AutoTokenizer, which optimizes your input and passes it to the language model efficiently. It does so by converting your text input to “tokens”, which is how the model interprets the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "aQ6EyUagCWDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model (download on first run and reference local installation for consequent runs)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BlRrlXFWCd_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Chat ###\n",
        "Now that you're all set up, let's start chatting!\n",
        "\n",
        "There are several things you'll do to have an effective conversation with your chatbot.\n",
        "\n",
        "Before interacting with your model, you need to initialize an object where you can store your conversation history.\n",
        "\n",
        "* Initialize object to store conversation history\n",
        "\n",
        "Afterward, you'll do the following for each interaction with the model:\n",
        "\n",
        "* Encode conversation history as a string\n",
        "* Fetch prompt from user\n",
        "* Tokenize (optimize) prompt\n",
        "* Generate output from the model using prompt and history\n",
        "* Decode output\n",
        "* Update conversation history"
      ],
      "metadata": {
        "id": "bw1wnMF6CsXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.1: Keeping track of conversation history ###\n",
        "\n",
        "The conversation history is important when interacting with a chatbot because the chatbot will also reference the previous conversations when generating output.\n",
        "\n",
        "For your simple implementation in Python, you may use a list. Per the Hugging Face implementation, you will use this list to store the conversation history as follows:\n",
        "\n",
        "conversation_history\n",
        "\n",
        "```python\n",
        "[input_1, output_1, input_2, output_2, …]\n",
        "```\n",
        "\n",
        "Let's initialize this list before any conversations occur."
      ],
      "metadata": {
        "id": "VICcDowMC6D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []"
      ],
      "metadata": {
        "id": "P4dLwHI-DOjz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.2: Encoding the conversation history ###\n",
        "\n",
        "During each interaction, you will pass your conversation history to the model along with your input so that it may also reference the previous conversation when generating the next answer.\n",
        "\n",
        "The transformers library function you are using expects to receive the conversation history as a string, with each element separated by the newline character '\\n'. Thus, you create such a string.\n",
        "\n",
        "You'll use the join() method in Python to do exactly that. (Initially, your history_string will be an empty string, which is okay, and will grow as the conversation goes on)."
      ],
      "metadata": {
        "id": "ogKs3HkiDY4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_string = \"\\n\".join(conversation_history)"
      ],
      "metadata": {
        "id": "W2MwRvYaDdA6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.3: Fetch prompt from user ###\n",
        "\n",
        "Before you start building a simple terminal chatbot, let's look at an example of the input:"
      ],
      "metadata": {
        "id": "sC5Er938DgfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text =\"hello, how are you doing?\""
      ],
      "metadata": {
        "id": "9tyhgGMnDkO8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.4: Tokenization of user prompt and chat history ###\n",
        "\n",
        "Tokens in NLP are individual units or elements that text or sentences are divided into. Tokenization or vectorization is the process of converting tokens into numerical representations.\n",
        "\n",
        "In NLP tasks, you often use the encode_plus method from the tokenizer object to perform tokenization and vectorization. Let's encode your inputs (prompt & chat history) as tokens so that you may pass them to the model."
      ],
      "metadata": {
        "id": "d6QPiAOPDtJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cibreeRXDzEX",
        "outputId": "f7d74188-fa0e-448a-9878-43c4770fa24a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[1710,   86,   19,  544,  366,  304,  929,   38]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about tokens and their associated pretrained vocabulary files, you can explore the pretrained_vocab_files_map attribute. This attribute provides a mapping of pretrained models to their corresponding vocabulary files."
      ],
      "metadata": {
        "id": "hd309_xYEFun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pretrained_vocab_files_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyf-BrdVD8D6",
        "outputId": "f4ae5062-9de3-43c8-cc1e-7fa7df7091ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.5: Generate output from the model ###\n",
        "\n",
        "Now that you have your inputs ready, both past and present inputs, you can pass them to the model and generate a response. According to the documentation, you can use the generate() function and pass the inputs as keyword arguments (kwargs)."
      ],
      "metadata": {
        "id": "hTUiC0dTEMOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO1nu7y5EQH_",
        "outputId": "0f4b0c1f-9913-4025-a977-36d79d4f73bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   1,  281,  476,  929,  731,   21,  281,  632,  929,  712,  731,   21,\n",
            "          855,  366,  304,   38,  946,  304,  360,  463, 5459, 7930,   38,    2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great - now you have your outputs! However, the current output outputs is also a dictionary and contains tokens, not words in plaintext.\n",
        "\n",
        "Therefore, you just need to decode the first index of outputs to see the response in plaintext.\n",
        "\n",
        "Please note that the model used in this project is a basic, lightweight version, not intended for handling complex queries. For more advanced and robust LLMs, you can explore a wide range of options at huggingface.com."
      ],
      "metadata": {
        "id": "Z7l47G1YEZ0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.6: Decode output ###\n",
        "\n",
        "You may decode the output using tokenizer.decode(). This is known as \"detokenization\" or \"reconstruction\". It is the process of combining or merging individual tokens back into their original form, to reconstruct the original text or sentence."
      ],
      "metadata": {
        "id": "NJ2O9blDEe4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkJgMxcEEkDI",
        "outputId": "fbabc1c7-8986-42aa-ef94-afab547de7ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm doing well. I am doing very well. How are you? Do you have any hobbies?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.7: Update conversation history ###\n",
        "\n",
        "All you need to do here is add both the input and response to conversation_history in plaintext."
      ],
      "metadata": {
        "id": "qastc3bjH1yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history.append(input_text)\n",
        "conversation_history.append(response)\n",
        "print(conversation_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA0sanlrH5HV",
        "outputId": "b6154d71-0db0-4592-dc58-b8711d94e621"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello, how are you doing?', \"I'm doing well. I am doing very well. How are you? Do you have any hobbies?\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Repeat ###\n",
        "\n",
        "You have gone through all the steps of interacting with your chatbot. Now, you can put everything in a loop and run a whole conversation!"
      ],
      "metadata": {
        "id": "eurfeJ4mH-pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    # Create conversation history string\n",
        "    history_string = \"\\n\".join(conversation_history)\n",
        "    # Get the input data from the user\n",
        "    input_text = input(\"> \")\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs)\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    print(response)\n",
        "    # Add interaction to conversation history\n",
        "    conversation_history.append(input_text)\n",
        "    conversation_history.append(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_r_NseQFIDjX",
        "outputId": "58012e99-de9a-493b-9787-1724ae8d9a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> How are you?\n",
            "That's great! I'm doing pretty well as well. What hobbies do you have?\n"
          ]
        }
      ]
    }
  ]
}