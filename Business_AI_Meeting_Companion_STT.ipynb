{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/C67HRRNrjnfR22OREt4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Building-Generative-AI-Powered-Applications-with-Python/blob/main/Business_AI_Meeting_Companion_STT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Objectives ###\n",
        "\n",
        "After finishing this lab, you will able to:\n",
        "\n",
        "* Create a Python script to generate text using a model from the Hugging Face Hub, identify some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.\n",
        "* Use OpenAI's Whisper technology to convert lecture recordings into text, accurately.\n",
        "* Implement IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.\n",
        "* Create an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators."
      ],
      "metadata": {
        "id": "BKIAziYIi3Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the environment ###\n",
        "\n",
        "Let's start with setting up the environment by creating a Python virtual environment and installing the required libraries, using the following commands in the terminal:"
      ],
      "metadata": {
        "id": "zZh6-od2i_X7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPAmluENiyEz"
      },
      "outputs": [],
      "source": [
        "!pip3 install virtualenv\n",
        "!virtualenv my_env # create a virtual environment my_env\n",
        "!source my_env/bin/activate # activate my_env"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, install the required libraries in the environment (this will take time ☕️☕️):"
      ],
      "metadata": {
        "id": "GPwLn_iqjRtk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfwyIprejSmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required libraries in my_env\n",
        "!pip install transformers==4.41.0 torch==2.6.0 gradio==5.23.2 langchain==0.3.25 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.28.1"
      ],
      "metadata": {
        "id": "rTJ0YjdsjUTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update"
      ],
      "metadata": {
        "id": "EychciJZmKFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install ffmpeg -y"
      ],
      "metadata": {
        "id": "fmbhZc9rmPX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# URL of the audio file to be downloaded\n",
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3\"\n",
        "# Send a GET request to the URL to download the file\n",
        "response = requests.get(url)\n",
        "# Define the local file path where the audio file will be saved\n",
        "audio_file_path = \"downloaded_audio.mp3\"\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # If successful, write the content to the specified local file path\n",
        "    with open(audio_file_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(\"File downloaded successfully\")\n",
        "else:\n",
        "    # If the request failed, print an error message\n",
        "    print(\"Failed to download the file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3liPo0nZmXIV",
        "outputId": "53d442a6-f755-46f8-c6eb-91ff82743943"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "# Initialize the speech-to-text pipeline from Hugging Face Transformers\n",
        "# This uses the \"openai/whisper-tiny.en\" model for automatic speech recognition (ASR)\n",
        "# The `chunk_length_s` parameter specifies the chunk length in seconds for processing\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-tiny.en\",\n",
        "  chunk_length_s=30,\n",
        ")\n",
        "# Define the path to the audio file that needs to be transcribed\n",
        "sample = \"sample_data/downloaded_audio.mp3\"\n",
        "# Perform speech recognition on the audio file\n",
        "# The `batch_size=8` parameter indicates how many chunks are processed at a time\n",
        "# The result is stored in `prediction` with the key \"text\" containing the transcribed text\n",
        "prediction = pipe(sample, batch_size=8)[\"text\"]\n",
        "# Print the transcribed text to the console\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1jb-K5cmc5G",
        "outputId": "5b320bff-b698-446a-ddbf-978b5f6ffbac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, I want to speak fast because I want to test four speech-to-text applications. Today, whether it's sunny, with a slight breeze, making it perfect for outdoor activity, later I plan for a busy local part, maybe even a picnic. The test is designed to assess the accuracy and responsiveness of the speech-to-text feature. Thank you for participating in this test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio interface - Creating a simple demo ###\n",
        "\n",
        "Through this project, we will create different LLM applications with Gradio interface. Let's get familiar with Gradio by creating a simple app:\n",
        "\n",
        "Still in the project directory, create a Python file and name it hello.py.\n",
        "\n",
        "Open hello.py, paste the following Python code and save the file."
      ],
      "metadata": {
        "id": "XFpSPKmdWslm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "zcDRSJtkW8p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "BIx8y2fiW1dZ",
        "outputId": "995e008e-0ddd-40ab-aceb-6df5662e7fc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://607c2d84d1ce3db209.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://607c2d84d1ce3db209.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code creates a gradio.Interface called demo. It wraps the greet function with a simple text-to-text user interface that you could interact with.\n",
        "\n",
        "The gradio.Interface class is initialized with 3 required parameters:\n",
        "\n",
        "fn: the function to wrap a UI around\n",
        "inputs: which component(s) to use for the input (e.g. “text”, “image” or “audio”)\n",
        "outputs: which component(s) to use for the output (e.g. “text”, “image” or “label”)\n",
        "The last line demo.launch() launches a server to serve our demo."
      ],
      "metadata": {
        "id": "zKMnS7tWXRg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Creating audio transcription app ###\n",
        "\n",
        "Create a new python file speech2text_app.py"
      ],
      "metadata": {
        "id": "P-qcby0UXYdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise: Complete the transcript_audio function. ####\n",
        "\n",
        "From the step1: fill the missing parts in transcript_audio function."
      ],
      "metadata": {
        "id": "zduzE4NEXdRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "# Function to transcribe audio using the OpenAI Whisper model\n",
        "def transcript_audio(audio_file):\n",
        "    # Initialize the speech recognition pipeline\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=\"openai/whisper-tiny.en\",\n",
        "        chunk_length_s=30,\n",
        "    )\n",
        "    # Transcribe the audio file and return the result\n",
        "    result = pipe(audio_file, batch_size=8)[\"text\"]\n",
        "    return result\n",
        "# Set up Gradio interface\n",
        "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")  # Audio input\n",
        "output_text = gr.Textbox()  # Text output\n",
        "# Create the Gradio interface with the function, inputs, and outputs\n",
        "iface = gr.Interface(fn=transcript_audio,\n",
        "                     inputs=audio_input, outputs=output_text,\n",
        "                     title=\"Audio Transcription App\",\n",
        "                     description=\"Upload the audio file\")\n",
        "# Launch the Gradio app\n",
        "iface.launch(server_name=\"0.0.0.0\", server_port=7861)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "k3V5v7JtXpT5",
        "outputId": "dd0af6a8-6c13-4ef2-eccc-b920d164cab3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2b17d0ce23ccc71d7d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2b17d0ce23ccc71d7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the sample audio file we've provided by right-clicking on it in the file explorer and selecting \"Download.\" Once downloaded, you can upload this file to the app. Alternatively, feel free to choose and upload any MP3 audio file from your local computer."
      ],
      "metadata": {
        "id": "hoNZDenrX0Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Integrating LLM: Using Llama 3 in WatsonX as LLM ###\n",
        "\n",
        "#### Running simple LLM ####\n",
        "\n",
        "Let's start by generating text with LLMs. Create a Python file and name it simple_llm.py. You can proceed by clicking the link below or by referencing the accompanying image.\n",
        "\n",
        "Here's how the code works:\n",
        "\n",
        "* Setting up credentials: The credentials needed to access IBM's services are pre-arranged by the Skills Network team, so you don't have to worry about setting them up yourself.\n",
        "\n",
        "* Specifying parameters: The code then defines specific parameters for the language model. 'MAX_NEW_TOKENS' sets the limit on the number of words the model can generate in one go. 'TEMPERATURE' adjusts how creative or predictable the generated text is.\n",
        "\n",
        "* Setting up Llama 3 model: Next, the LLAMA3 model is set up using a model ID, the provided credentials, chosen parameters, and a project ID.\n",
        "\n",
        "* Creating an object for Llama 3: The code creates an object named llm, which is used to interact with the Llama 3 model. A model object, LLAMA3_model, is created using the Model class, which is initialized with a specific model ID, credentials, parameters, and project ID. Then, an instance of WatsonxLLM is created with LLAMA3_model as an argument, initializing the language model hub llm object.\n",
        "\n",
        "* Generating and printing response: Finally, 'llm' is used to generate a response to the question, \"How to read a book effectively?\" The response is then printed out."
      ],
      "metadata": {
        "id": "tWKgyXQ4X_-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ibm-watson-machine-learning"
      ],
      "metadata": {
        "id": "DCQiP6UVY0m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "trPz3uR1a752"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "my_credentials = {\n",
        "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
        "}\n",
        "params = {\n",
        "        GenParams.MAX_NEW_TOKENS: 700, # The maximum number of tokens that the model can generate in a single run.\n",
        "        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
        "    }\n",
        "LLAMA2_model = Model(\n",
        "        model_id= 'meta-llama/llama-3-2-11b-vision-instruct',\n",
        "        credentials=my_credentials,\n",
        "        params=params,\n",
        "        project_id=\"skills-network\",\n",
        "        )\n",
        "llm = WatsonxLLM(LLAMA2_model)\n",
        "print(llm(\"How to read a book effectively?\"))"
      ],
      "metadata": {
        "id": "bWRkvzZmYNOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Put them all together ###\n",
        "\n",
        "In this exercise, we'll set up a language model (LLM) instance, which could be IBM WatsonxLLM, HuggingFaceHub, or an OpenAI model. Then, we'll establish a prompt template. These templates are structured guides to generate prompts for language models, aiding in output organization (more info in langchain prompt template.\n",
        "\n",
        "Next, we'll develop a transcription function that employs the OpenAI Whisper model to convert speech-to-text. This function takes an audio file uploaded through a Gradio app interface (preferably in .mp3 format). The transcribed text is then fed into an LLMChain, which integrates the text with the prompt template and forwards it to the chosen LLM. The final output from the LLM is then displayed in the Gradio app's output textbox."
      ],
      "metadata": {
        "id": "LyKjOxZfbrJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import gradio as gr\n",
        "#from langchain.llms import OpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from transformers import pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "my_credentials = {\n",
        "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
        "}\n",
        "params = {\n",
        "        GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.\n",
        "        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
        "    }\n",
        "LLAMA2_model = Model(\n",
        "        model_id= 'meta-llama/llama-3-2-11b-vision-instruct',\n",
        "        credentials=my_credentials,\n",
        "        params=params,\n",
        "        project_id=\"skills-network\",\n",
        "        )\n",
        "llm = WatsonxLLM(LLAMA2_model)\n",
        "#######------------- Prompt Template-------------####\n",
        "temp = \"\"\"\n",
        "<s><<SYS>>\n",
        "List the key points with details from the context:\n",
        "[INST] The context : {context} [/INST]\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "pt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template= temp)\n",
        "prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)\n",
        "#######------------- Speech2text-------------####\n",
        "def transcript_audio(audio_file):\n",
        "    # Initialize the speech recognition pipeline\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=\"openai/whisper-tiny.en\",\n",
        "        chunk_length_s=30,\n",
        "    )\n",
        "    # Transcribe the audio file and return the result\n",
        "    transcript_txt = pipe(audio_file, batch_size=8)[\"text\"]\n",
        "    result = prompt_to_LLAMA2.run(transcript_txt)\n",
        "    return result\n",
        "#######------------- Gradio-------------####\n",
        "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")\n",
        "output_text = gr.Textbox()\n",
        "iface = gr.Interface(fn= transcript_audio,\n",
        "                    inputs= audio_input, outputs= output_text,\n",
        "                    title= \"Audio Transcription App\",\n",
        "                    description= \"Upload the audio file\")\n",
        "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
      ],
      "metadata": {
        "id": "EnUbnwp6bqBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion ###\n",
        "\n",
        "Congratulations on completing this project! You have now laid a solid foundation for leveraging powerful Language Models (LLMs) for speech-to-text generation tasks. Here's a quick recap of what you've accomplished:\n",
        "\n",
        "* Text generation with LLM: You've created a Python script to generate text using a model from the Hugging Face Hub, learned about some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.\n",
        "\n",
        "* Speech-to-Text conversion: Utilize OpenAI's Whisper technology to convert lecture recordings into text, accurately.\n",
        "\n",
        "* Content summarization: Implement IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.\n",
        "\n",
        "* User interface development: Create an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators."
      ],
      "metadata": {
        "id": "_nErZlk2cD4j"
      }
    }
  ]
}